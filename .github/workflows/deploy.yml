name: Deploy to S3 and Run EMR Job

on:
  push:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.8'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Package code and upload to S3
      uses: aws-actions/aws-s3-sync@v1
      with:
        args: |
          sync src/ s3://kan-emr-works/emr-repo/emr-pyspark/
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_REGION: us-east-1  # Your AWS region

    - name: Submit Spark job to EMR
      run: |
        aws emr add-steps \
            --cluster-id YOUR_CLUSTER_ID \
            --steps Type=Spark,Name="PySpark job",ActionOnFailure=CONTINUE,Args=[
                --py-files,s3://kan-emr-works/emr-repo/emr-pyspark/src/utils.py,
                s3://kan-emr-works/emr-repo/emr-pyspark/src/main.py,
                --deploy-mode,cluster
            ]
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_REGION: us-east-1  # Your AWS region
